import os

configfile: "config/demo.yaml"

dataset = config["dataset"]
procs = config["processors"]
dist_thresh = config["distance_threshold"]
subsample_size = config["subsample_size"]
silva_version = config["silva_version"]
seed = config["seed"]

srr_filename = config['srr_filename']
use_srr_list = False
if srr_filename and os.path.exists(srr_filename):
    use_srr_list = True
    with open(srr_filename, "r") as file:
        sra_list = [line.strip() for line in file]


rule targets:
    input:
        "paper/paper.pdf",
        "README.md",


rule download_silva:
    output:
        tar=temp(f"data/references/silva.seed_v{silva_version}.tgz"),
        fasta=temp(f"data/references/silva.seed_v{silva_version}.align"),
        tax=temp(f"data/references/silva.seed_v{silva_version}.tax"),
    params:
        outdir="data/references/",
        silva_v=silva_version,
    resources:
        mem_mb=config["download_silva"]["mem_mb"],
        time_min=config["download_silva"]["time_min"],
    threads: config["download_silva"]["threads"],
    shell:
        """
        #source /etc/profile.d/http_proxy.sh  # required for internet on the Great Lakes cluster
        wget -N -P {params.outdir} https://mothur.s3.us-east-2.amazonaws.com/wiki/silva.seed_v{params.silva_v}.tgz
        tar xzvf {output.tar} -C {params.outdir}
        """


rule process_silva:
    input:
        fasta=rules.download_silva.output.fasta,
        tax=rules.download_silva.output.tax,
    output:
        full="data/references/silva.seed.align",
        v4="data/references/silva.v4.align",
    conda: "envs/mothur.yaml",
    params:
        silva_v=silva_version,
        workdir="data/references/",
        full=f"data/references/silva.seed_v{silva_version}.pick.align",
        v4=f"data/references/silva.seed_v{silva_version}.pick.pcr.align",
        v4_start=11894,
        v4_end=25319
    log:
        "log/mothur/get_silva.log",
    threads: procs
    shell:
        """
        mothur "#set.logfile(name={log});
                set.dir(output={params.workdir}, input={params.workdir});
                get.lineage(fasta={input.fasta}, taxonomy={input.tax}, taxon=Bacteria);
                degap.seqs(fasta={params.full}, processors={threads});
                pcr.seqs(fasta={params.full}, start={params.v4_start}, end={params.v4_end}, keepdots=F, processors={threads})
                "
        mv {params.full} {output.full}
        mv {params.v4} {output.v4}
        """


rule download_rdp:
    output:
        tar=temp("data/references/trainset18_062020.pds.tgz"),
        fasta="data/references/rdp.fasta",
        tax="data/references/rdp.tax",
    params:
        outdir="data/references/",
        fasta="data/references/trainset18_062020.pds/trainset18_062020.pds.fasta",
        tax="data/references/trainset18_062020.pds/trainset18_062020.pds.tax",
        url="https://mothur.s3.us-east-2.amazonaws.com/wiki/trainset18_062020.pds.tgz",
    shell:
        """
        #source /etc/profile.d/http_proxy.sh  # required for internet on the Great Lakes cluster
        wget -N -P {params.outdir} {params.url}
        tar xvzf {output.tar} -C {params.outdir}
        mv {params.fasta} {output.fasta}
        mv {params.tax} {output.tax}
        rm -rf {params.outdir}/trainset18_*.pds/
        """


checkpoint download_demo_data:
    output:
        zip=temp("data/miseqsopdata.zip"),
        outdir=directory("data/raw/"),
    shell:
        """
        wget -N -P data/ https://mothur.s3.us-east-2.amazonaws.com/wiki/miseqsopdata.zip
        unzip {output.zip} -d {output.outdir}
        mv {output.outdir}/MiSeq_SOP/*.fast* {output.outdir}
        rm -rf {output.outdir}/__MACOSX
        rm -rf {output.outdir}/MiSeq_SOP
        """


rule download_sra_data:
    input:
        txt=srr_filename,
    output:
        fastq=expand("data/raw/{{SRA}}_{i}.fastq", i=(1, 2)),
    params:
        sra="{SRA}",
        outdir="data/raw",
    shell:
        """
        #source /etc/profile.d/http_proxy.sh  # required for internet on the Great Lakes cluster
        prefetch {params.sra}
        fasterq-dump --split-files {params.sra} -O {params.outdir}
        """


def list_demo_fastq(wildcards):
    checkpoint_output = checkpoints.download_demo_data.get(**wildcards).output.outdir
    new_wildcards = glob_wildcards("data/raw/{sample}_L001_R{read}_001.fastq")
    return expand(
        "data/raw/{sample}_L001_R{read}_001.fastq",
        sample=new_wildcards.sample,
        read=new_wildcards.read,
    )

def list_fastq_files(wildcards):
    if use_srr_list:
        fastq_files = expand("data/raw/{SRA}_{i}.fastq", SRA = sra_list, i = [1,2])
    else:
        checkpoint_output = checkpoints.download_demo_data.get(**wildcards).output.outdir
        new_wildcards = glob_wildcards("data/raw/{sample}_L001_R{read}_001.fastq")
        fastq_files = expand(
            "data/raw/{sample}_L001_R{read}_001.fastq",
            sample=new_wildcards.sample,
            read=new_wildcards.read,
        )
    return fastq_files

rule process_data:
    input:
        fastq=list_fastq_files, 
        silva_v4=rules.process_silva.output.v4,
        rdp_fasta=rules.download_rdp.output.fasta,
        rdp_tax=rules.download_rdp.output.tax,
    output:
        files="data/mothur/{dataset}.files",
        count_table="data/processed/{dataset}.count_table",
        tax="data/processed/{dataset}.tax",
        fasta="data/processed/{dataset}.fasta",
    conda: "envs/mothur.yaml",
    params:
        inputdir="data/raw/",
        workdir="data/mothur/",
        files="data/mothur/{dataset}.paired.files",
        contigs_fna="data/mothur/{dataset}.trim.contigs.fasta",
        contigs_count="data/mothur/{dataset}.contigs.count_table",
        screen_start=1968,
        screen_end=11550,
        maxambig=0,
        maxlength=275,
        maxhomop=8,
        diffs=2,
        tax="data/mothur/{dataset}.trim.contigs.unique.good.filter.unique.precluster.rdp.wang.pick.taxonomy",
        fasta="data/mothur/{dataset}.trim.contigs.unique.good.filter.unique.precluster.fasta",
        count_table="data/mothur/{dataset}.trim.contigs.unique.good.filter.unique.precluster.denovo.vsearch.count_table",
    log:
        "log/mothur/process_data_{dataset}.log",
    threads: procs
    shell:
        """
        mothur '#set.logfile(name={log});
            set.dir(input={params.inputdir}, output={params.workdir});
            make.file(inputdir={params.inputdir}, type=fastq, prefix={wildcards.dataset});
            rename.file(input={params.files}, new={output.files});
            make.contigs(inputdir={params.inputdir}, file={output.files}, processors={threads}, maxambig={params.maxambig}, maxlength={params.maxlength}, maxhomop={params.maxhomop});
            set.dir(input={params.workdir}, output={params.workdir});
            unique.seqs(fasta={params.contigs_fna}, count={params.contigs_count});
            align.seqs(fasta=current, reference={input.silva_v4}, processors={threads});
            screen.seqs(fasta=current, count=current, start={params.screen_start}, end={params.screen_end});
            filter.seqs(fasta=current, vertical=T, trump=.);
            unique.seqs(fasta=current, count=current);
            pre.cluster(fasta=current, count=current, diffs={params.diffs});
            chimera.vsearch(fasta=current, count=current, dereplicate=T);
            classify.seqs(fasta=current, count=current, reference={input.rdp_fasta}, taxonomy={input.rdp_tax});
            remove.lineage(fasta=current, count=current, taxonomy=current, taxon=Chloroplast-Mitochondria-unknown-Archaea-Eukaryota)
            '
        
        mv {params.tax} {output.tax}
        mv {params.fasta} {output.fasta}
        mv {params.count_table} {output.count_table}
        """

rule calc_dists:
    input:
        fasta=rules.process_data.output.fasta,
    output:
        column="data/processed/{dataset}.dist",
    conda: "envs/mothur.yaml",
    params:
        outdir="data/processed/",
        cutoff=dist_thresh,
    log:
        "log/mothur/calc_dists_{dataset}.log",
    threads: procs
    shell:
        """
        mothur '#set.logfile(name={log}); set.dir(output={params.outdir});
            dist.seqs(fasta={input.fasta}, cutoff={params.cutoff}, processors={threads}) 
            '
        """


rule cluster_OTUs:
    input:
        dist=rules.calc_dists.output.column,
        count_table=rules.process_data.output.count_table,
    output:
        list="results/{dataset}.opti_mcc.list",
        sensspec="results/{dataset}.opti_mcc.sensspec",
        steps="results/{dataset}.opti_mcc.steps",
    conda: "envs/mothur.yaml",
    params:
        outdir="results/",
        cutoff=dist_thresh,
        seed=seed
    log:
        "log/mothur/cluster_{dataset}.log",
    benchmark:
        "benchmarks/mothur/cluster_{dataset}.txt"
    threads: procs
    shell:
        """
        mothur '#set.logfile(name={log}); set.dir(output={params.outdir});
            set.seed(seed={params.seed});
            set.current(processors={threads});
            cluster(column={input.dist}, count={input.count_table}, cutoff={params.cutoff}) 
            '
        """


rule get_shared:
    input:
        list=rules.cluster_OTUs.output.list,
        count_table=rules.process_data.output.count_table,
        tax=rules.process_data.output.tax,
    output:
        shared="results/{dataset}.opti_mcc.shared",
        tax='results/{dataset}.opti_mcc.0.03.cons.taxonomy'
    conda: "envs/mothur.yaml",
    params:
        outdir="results/",
        label=dist_thresh,
    log:
        "log/mothur/get_shared_{dataset}.log",
    shell:
        """
        mothur '#set.logfile(name={log}); set.dir(output={params.outdir});
            make.shared(list={input.list}, count={input.count_table}, label={params.label});
            classify.otu(list=current, count=current, taxonomy={input.tax}, label={params.label});
            count.groups(shared=current)
            '
        """


rule calc_diversity:
    input:
        shared=rules.get_shared.output.shared,
        tax=rules.get_shared.output.tax,
    output:
        rare="results/{dataset}.opti_mcc.groups.rarefaction",
        div="results/{dataset}.opti_mcc.groups.ave-std.summary"
    conda: "envs/mothur.yaml",
    params:
        outdir='results/',
        subsample_size=subsample_size # TODO: use result of count_groups to calc this
    log:
        "log/mothur/calc_diversity_{dataset}.log"
    shell:
        """
        mothur '#set.logfile(name={log});
        set.dir(output={params.outdir}, seed=19760620);
        rename.file(taxonomy={input.tax}, shared={input.shared});
        sub.sample(shared={input.shared}, size={params.subsample_size});
        rarefaction.single(shared={input.shared}, calc=sobs, freq=100);
        summary.single(shared={input.shared}, calc=nseqs-coverage-invsimpson-shannon-sobs, subsample={params.subsample_size})
        '
        """

rule plot_diversity:
    input:
        tsv=rules.calc_diversity.output.div,
        R='workflow/scripts/plot_diversity.R'
    output:
        alpha='figures/{dataset}_alpha-diversity.pdf'
    conda: 'envs/Rtidy.yaml'
    log: 'log/plot_diversity_{dataset}.log'
    script:
        'scripts/plot_diversity.R'
    

rule render_paper:
    input:
        Rmd="paper/paper.Rmd",
        R="workflow/scripts/render_rmd.R",
        figures=expand(rules.plot_diversity.output, dataset = dataset)
    output:
        pdf="paper/paper.pdf",
        md="paper/paper.md",
    conda: "envs/Rtidy.yaml",
    log: "log/render_paper.log"
    script:
        "scripts/render_rmd.R"


rule render_readme:
    input:
        Rmd="README.Rmd",
        R="workflow/scripts/render_rmd.R",
        image='figures/rulegraph_demo.png'
    output:
        md="README.md",
        html=temp("README.html"),
    conda: "envs/Rtidy.yaml",
    log: "log/render_readme.log"
    script:
        "scripts/render_rmd.R"
